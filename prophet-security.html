<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Prophet Security | Brent Benson</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
    :root {
      --blueprint:#005a9c;
      --graphite:#2d2d2d;
      --paper:#f5f5f0;
    }
    *{box-sizing:border-box;margin:0;padding:0;}
    body{
      font-family:system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",sans-serif;
      background:var(--paper);
      color:var(--graphite);
      line-height:1.6;
    }
    header{
      position:sticky;
      top:0;
      z-index:10;
      background:#fff;
      border-bottom:1px solid rgba(0,0,0,0.08);
    }
    .nav{
      max-width:1024px;
      margin:0 auto;
      padding:0.75rem 1.5rem;
      display:flex;
      align-items:center;
      justify-content:space-between;
    }
    .logo{
      font-weight:700;
      letter-spacing:0.08em;
      font-size:0.9rem;
      text-transform:uppercase;
      padding:0.25rem 0.5rem;
      border:1px solid var(--graphite);
    }
    .nav ul{
      list-style:none;
      display:flex;
      gap:1rem;
      font-size:0.9rem;
    }
    .nav a{
      text-decoration:none;
      color:var(--graphite);
      padding:0.25rem 0.5rem;
      border-radius:999px;
      transition:background 0.15s ease,color 0.15s ease;
    }
    .nav a:hover{
      background:var(--blueprint);
      color:#fff;
    }

    main{
      max-width:1024px;
      margin:0 auto;
      padding:2rem 1.5rem 3rem;
    }

    .kicker{
      font-size:0.78rem;
      text-transform:uppercase;
      letter-spacing:0.18em;
      color:#777;
      margin-bottom:0.9rem;
    }
    h1{
      font-size:1.9rem;
      margin-bottom:0.5rem;
    }
    .intro{
      max-width:640px;
      font-size:0.98rem;
      margin-bottom:2rem;
    }

    .section-title{
      font-size:1.4rem;
      margin-bottom:0.4rem;
    }

    .two-column{
      display:grid;
      grid-template-columns:minmax(0,1.3fr) minmax(0,1fr);
      gap:2rem;
      align-items:flex-start;
      margin-bottom:2.5rem;
    }
    .card-list{
      display:grid;
      gap:1rem;
    }
    .card{
      background:#fff;
      border-radius:0.85rem;
      padding:1rem 1.1rem;
      border:1px solid rgba(0,0,0,0.06);
    }
    .pill{
      display:inline-flex;
      font-size:0.7rem;
      text-transform:uppercase;
      letter-spacing:0.12em;
      padding:0.15rem 0.6rem;
      border-radius:999px;
      border:1px solid rgba(0,0,0,0.16);
      margin-bottom:0.35rem;
      color:#555;
    }
    .card h2{
      font-size:1.05rem;
      margin-bottom:0.25rem;
    }
    .card h3{
      font-size:1.02rem;
      margin-bottom:0.25rem;
    }
    .meta{
      font-size:0.8rem;
      color:#777;
      margin-top:0.35rem;
    }

    table{
      width:100%;
      border-collapse:collapse;
      font-size:0.9rem;
    }
    table th, table td{
      border:1px solid rgba(0,0,0,0.1);
      padding:0.6rem 0.6rem;
      vertical-align:top;
      text-align:left;
    }
    table th{
      background:#f0f0f0;
      font-weight:600;
    }

    .title-row {
      display:flex;
      align-items:flex-start;
      justify-content:space-between;
      gap:1.5rem;
      margin-bottom:2rem;
    }
    .title-text {
      flex:1.4;
    }
    .title-logo {
      flex:0.8;
      display:flex;
      align-items:flex-start;
      justify-content:flex-end;
    }
    .rf-logo {
      max-width:220px;
      height:auto;
      opacity:0.95;
    }

    @media(max-width:768px){
      .two-column{
        grid-template-columns:1fr;
      }
      .nav{
        flex-direction:column;
        align-items:flex-start;
        gap:0.5rem;
      }
      .nav ul{
        flex-wrap:wrap;
        justify-content:flex-start;
      }
      .title-row{
        flex-direction:column;
        align-items:flex-start;
      }
      .title-logo{
        justify-content:flex-start;
      }
      .rf-logo{
        max-width:200px;
      }
    }
  </style>
</head>
<body>
<header>
  <nav class="nav">
    <div class="logo">BB</div>
    <ul>
      <li><a href="index.html">Home</a></li>
      <li><a href="work.html">Work</a></li>
      <li><a href="life.html">Life</a></li>
      <li><a href="notes.html">Notes</a></li>
    </ul>
  </nav>
</header>

<main>
  <div class="kicker">Target company</div>

  <div class="title-row">
    <div class="title-text">
      <h1>Prophet Security: Agentic SOC territory view</h1>
      <p class="intro">
        This page shows how I think about Prophet Security and an Agentic SOC motion in Minnesota and Iowa using a Gap Selling lens:
        the problems large SOCs and MDR customers face today, the business impact of those problems, and the root causes that keep
        them there, plus how I would build a focused account plan.
      </p>
    </div>
    <div class="title-logo">
      <!-- update src to your actual Prophet logo file path -->
      <img src="images/prophet-security-logo.png" alt="Prophet Security logo" class="rf-logo">
    </div>
  </div>

  <!-- Section 1: Problems / impact / causes (Agentic SOC) -->
  <section>
    <div class="kicker">Problems, impact, causes</div>
    <h2 class="section-title">How an Agentic SOC from Prophet maps to customer gaps</h2>
    <p class="intro">
      Prophet’s Agentic SOC model fits naturally into Gap Selling-style problems, impacts, and root causes across core SOC operations,
      MDR shortcomings, and the realities of large, tool-heavy security programs.
    </p>

    <!-- Core SOC problems for Prophet -->
    <h3 class="section-title" style="font-size:1.1rem; margin-top:1.5rem;">Core SOC problems Prophet addresses</h3>
    <div style="overflow-x:auto; margin-top:0.5rem;">
      <table>
        <thead>
          <tr>
            <th>Problem (customer’s world)</th>
            <th>Business impact</th>
            <th>Likely root causes</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Alert fatigue and high manual investigation load in the SOC</strong></td>
            <td>
              Analysts spend 20–60 minutes per alert, limiting throughput to tens of alerts per analyst per day. Real deployments have seen
              average investigation time drop from about 30 minutes to about 5 minutes per alert, yielding roughly 10x SOC throughput and
              double-digit days of analyst time saved in a single quarter. Reclaiming even 20–30 percent of analyst time effectively returns
              tens of thousands of dollars per analyst per year.
            </td>
            <td>
              Highly manual triage and investigation where analysts pivot between SIEM, EDR, identity, and cloud tools to collect context,
              brittle SOAR playbooks that cover only a subset of scenarios, and a historic focus on adding tools and telemetry without
              deep automation and correlation.
            </td>
          </tr>
          <tr>
            <td><strong>High MTTR and dwell time for real threats</strong></td>
            <td>
              Longer mean time to respond increases the likelihood and impact of successful breaches. AI SOC agents can reduce MTTR by
              multiples, turning investigations that took hours into minutes. Each extra hour or day of dwell time raises potential
              incident cost across investigation, containment, recovery, and business disruption.
            </td>
            <td>
              Investigation bottlenecks with humans performing serial investigations and queuing alerts, high false-positive volume
              that consumes analyst attention, and limited use of autonomous investigation and response that can act immediately when
              an alert fires.
            </td>
          </tr>
          <tr>
            <td><strong>Large volume of false positives consuming SOC capacity</strong></td>
            <td>
              SOCs can receive thousands of alerts per day; a large share are false positives. In one example, an AI SOC processed
              thousands of alerts in a month but escalated only a handful of true critical issues, letting the agent close benign alerts
              autonomously. Reducing false positives at scale cuts wasted analyst hours and telemetry costs and lowers the risk that
              real threats are missed amid noise.
            </td>
            <td>
              Detection rules tuned defensively to avoid misses and vendor defaults generating excessive noise, limited contextual
              enrichment at triage time, and no system that can safely and transparently close benign alerts with a full audit trail,
              so teams err on the side of manual review.
            </td>
          </tr>
          <tr>
            <td><strong>SOC headcount does not scale with alert volume and tooling</strong></td>
            <td>
              Average fully loaded analyst cost is high, and increasing coverage by adding headcount can require hundreds of thousands
              to millions of dollars over a few years, plus turnover cost when analysts leave. An Agentic SOC lets teams handle far more
              alerts with the same or smaller team; customers report AI effectively “paying for itself” within a year via investigation
              time saved and avoided headcount.
            </td>
            <td>
              Security leaders have treated headcount as the primary lever for growth, many SOC tools assume humans are the core processing
              engine, and a lack of clear ROI models for AI-driven operations has delayed serious investment in automation.
            </td>
          </tr>
          <tr>
            <td><strong>High analyst burnout and attrition due to repetitive work</strong></td>
            <td>
              Industry reports show analysts spending most of their day on repetitive, manual tasks, which drives burnout and turnover.
              Replacing an experienced analyst can cost 1.5–2x their salary. With an agent handling the tedious parts of investigations,
              analysts can move to higher-value work like threat hunting and detection engineering, improving satisfaction and retention.
            </td>
            <td>
              Job design centered on “alert chasing” and low-value manual investigation, limited investment in automation that can take
              over the tedious parts of the job, and culture and tooling that do not prioritize analyst experience or sustainable workloads.
            </td>
          </tr>
          <tr>
            <td><strong>Underutilized security tool stack and bloated SIEM costs</strong></td>
            <td>
              Many organizations pay heavily for SIEM ingestion and storage yet use only a fraction of that data for investigations.
              Case studies show SIEM spend reduced dramatically by letting an AI agent pull evidence directly from native systems instead
              of centralizing everything. Overlapping tools and duplicative workflows increase license costs and complexity, lowering ROI.
            </td>
            <td>
              Tool sprawl as point solutions were added over time without a unifying investigation layer, SIEM-centric architectures that
              centralize everything by default rather than querying on demand, and a lack of metrics that tie data retention to concrete
              investigation value.
            </td>
          </tr>
          <tr>
            <td><strong>Lack of transparency and trust in AI recommendations</strong></td>
            <td>
              If analysts do not trust AI outputs, they redo investigations manually, erasing time savings and keeping MTTR high.
              Low trust reduces adoption and undermines the ROI promised on AI investments. A transparent AI SOC that shows every
              step and piece of evidence lets analysts “grade the homework” instead of redoing it.
            </td>
            <td>
              Black-box AI tools that provide conclusions without showing steps or evidence, early AI focused on simple triage scoring
              instead of full investigations, and no consistent framework for measuring AI accuracy and establishing human-in-the-loop
              feedback.
            </td>
          </tr>
          <tr>
            <td><strong>Incomplete or inconsistent SOC metrics and reporting</strong></td>
            <td>
              Without clear metrics like MTTD, MTTR, false positive rates, and investigation throughput, leaders cannot quantify risk
              or ROI, which makes it hard to justify budgets and improvements. Poor metrics lead to misallocated investments, often
              toward more tools instead of efficiency and depth of investigation.
            </td>
            <td>
              Legacy reporting focused on volume and activity (alerts processed, tickets closed) rather than outcomes and efficiency,
              fragmented tools and data that make consolidation hard, and no purpose-built AI SOC layer that exposes consistent
              end-to-end investigation telemetry.
            </td>
          </tr>
        </tbody>
      </table>
    </div>

    <!-- MDR-specific problems -->
    <h3 class="section-title" style="font-size:1.1rem; margin-top:1.75rem;">MDR-specific problems an Agentic SOC can solve</h3>
    <div style="overflow-x:auto; margin-top:0.5rem;">
      <table>
        <thead>
          <tr>
            <th>Problem (MDR customer’s world)</th>
            <th>Business impact</th>
            <th>Likely root causes</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>MDR ignores or downplays low and medium alerts where early compromise lives</strong></td>
            <td>
              Human MDR teams focus on high severity alerts to stay afloat, so subtle identity or SaaS signals early in an attack
              are often never investigated. This increases breach likelihood and dwell time, turning small incidents into large,
              costly events while the organization still pays full MDR fees.
            </td>
            <td>
              Human-only MDR models cannot economically investigate every low-severity alert across EDR, identity, SaaS, cloud,
              and email, and services are optimized for scalable, repeatable detections rather than deep, client-specific weak signals.
            </td>
          </tr>
          <tr>
            <td><strong>Loss of organizational context when incidents are outsourced to MDR</strong></td>
            <td>
              Alerts get closed as benign because MDR analysts do not know local changes, business processes, or legitimate but
              unusual behavior. Correlations across identity, cloud, and business apps are often missed, leaving gaps in detection
              and incident understanding.
            </td>
            <td>
              MDR analysts sit outside the organization and rarely have full access to internal context, and playbooks and detections
              are built for the “average customer” rather than tailored to each environment.
            </td>
          </tr>
          <tr>
            <td><strong>Limited support for custom detections and unique use cases</strong></td>
            <td>
              Mature security programs struggle to operationalize their custom detections through MDR and end up running parallel
              internal processes, which leads to duplicated effort, slower responses, and missed ROI on both MDR and internal investments.
            </td>
            <td>
              MDR offerings are designed to be standardized and scalable, prioritizing generic detection content over customer-specific
              detections, and contracts and tooling often make it hard to push bespoke rules into MDR workflows.
            </td>
          </tr>
          <tr>
            <td><strong>High recurring MDR cost without proportional reduction in internal workload</strong></td>
            <td>
              Many customers keep internal analysts “watching the watchers,” re-validating MDR alerts and doing internal investigations
              on top. They pay both MDR fees and internal headcount yet still struggle with alert fatigue.
            </td>
            <td>
              MDR was adopted as a way to “buy a SOC,” but internal complexity (cloud, SaaS, identity) continued to grow while MDR
              scope did not, and there is often no clear metric framework for how much work MDR actually removed.
            </td>
          </tr>
          <tr>
            <td><strong>Slow, ticket-driven back-and-forth during active incidents</strong></td>
            <td>
              During serious incidents, internal teams and MDR analysts often rely on slow ticket updates and scheduled calls,
              which can add hours or days to timelines and expand the blast radius of an incident.
            </td>
            <td>
              MDR processes were built around tickets, SLAs, and escalation paths, not around unified, shared investigative timelines,
              and MDR and customer teams often use different tools and data sources.
            </td>
          </tr>
          <tr>
            <td><strong>Limited transparency into how MDR reached conclusions</strong></td>
            <td>
              Security leaders struggle to audit MDR decisions or learn from incidents because they receive summaries rather than
              full step-by-step investigations, so internal teams often redo parts of the work.
            </td>
            <td>
              MDR workflows and tools are proprietary and optimized for internal efficiency rather than customer visibility, and
              reports are often abstracted for executives instead of exposing investigative detail.
            </td>
          </tr>
          <tr>
            <td><strong>Inflexible service model as environments expand</strong></td>
            <td>
              MDR coverage often lags where risk moves (identity, SaaS, OT), leaving blind spots where customers must still build
              monitoring and response on their own, creating a patchwork between MDR and internal coverage.
            </td>
            <td>
              MDRs were born in an endpoint-centric world and expanded gradually, and onboarding new data sources into MDR workflows
              is expensive and standardized rather than adaptive to each environment.
            </td>
          </tr>
          <tr>
            <td><strong>Difficulty measuring MDR’s true ROI vs an AI-augmented in-house SOC</strong></td>
            <td>
              Without granular metrics on investigation time, MTTR, false positives, and human hours saved, it is hard to prove MDR
              is more effective than investing in an AI-driven internal SOC, which makes renewal and transformation decisions hard.
            </td>
            <td>
              MDR reporting focuses on tickets handled and coverage rather than unit economics per investigation or saved analyst hour,
              and there is no internal framework to compare MDR output to what an AI SOC platform could do on the same alerts.
            </td>
          </tr>
        </tbody>
      </table>
    </div>

    <!-- Problems in large SOCs -->
    <h3 class="section-title" style="font-size:1.1rem; margin-top:1.75rem;">Problems in large SOCs Prophet can address</h3>
    <div style="overflow-x:auto; margin-top:0.5rem;">
      <table>
        <thead>
          <tr>
            <th>Problem (in a mature SOC)</th>
            <th>Business impact</th>
            <th>Likely root causes</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><strong>Alert volume outstrips analyst capacity, even with SIEM and SOAR</strong></td>
            <td>
              Large enterprises see thousands of alerts per day from many alert-generating tools, and even strong SOCs cannot
              investigate all of them thoroughly. At 30–60 minutes per investigation, that is hundreds of analyst hours per month,
              translating into significant fully loaded cost and higher breach risk.
            </td>
            <td>
              Stack evolution focused on adding more detection tools and telemetry, SOAR automating steps but still relying on humans
              to drive investigations, and SIEM optimized for aggregation and search rather than autonomously doing analyst work.
            </td>
          </tr>
          <tr>
            <td><strong>SIEM and data storage costs are high, but much of the data is under-used</strong></td>
            <td>
              Very large organizations ingest logs from dozens of tools; SIEM licensing and storage can be very expensive while a
              significant portion of data is rarely queried. Case studies show AI SOC platforms reducing SIEM-related spend by
              shifting to on-demand evidence collection.
            </td>
            <td>
              An “ingest everything” mindset where SIEM is treated as the only analysis layer, lack of targeted, on-demand data access
              models, and no clear mapping between data sources, investigations closed, and business value.
            </td>
          </tr>
          <tr>
            <td><strong>Tool sprawl and fragmented workflows across SIEM, EDR, IAM, cloud, and SaaS</strong></td>
            <td>
              Security teams often manage dozens of alert-generating tools with separate consoles. Investigations require pivoting
              across multiple systems, extending time per case and increasing the risk of missing cross-domain attack paths.
            </td>
            <td>
              Point solutions added over years to solve specific problems without a unifying investigation and orchestration layer,
              SOAR scripts built per tool rather than end-to-end, and SIEM correlating logs but still leaving humans to stitch together
              a coherent narrative.
            </td>
          </tr>
          <tr>
            <td><strong>MTTR remains high despite “good tools” and a staffed SOC</strong></td>
            <td>
              Even with SIEM and EDR, organizations struggle with MTTR because investigations queue behind other work. An Agentic SOC
              can cut investigation time sharply and increase throughput, directly reducing dwell time and incident scope.
            </td>
            <td>
              Investigations are still driven by humans pulling data and writing timelines, metrics focus on alerts handled instead of
              time from detection to remediation, and limited automation exists for medium severity issues.
            </td>
          </tr>
          <tr>
            <td><strong>Skilled analysts spend too much time on repetitive triage</strong></td>
            <td>
              High-cost senior analysts spend large portions of each day validating obvious false positives, copy-pasting evidence, and
              documenting investigations. AI SOC customers report reclaiming enough time that AI delivers the efficiency of a much larger team.
            </td>
            <td>
              No AI analyst taking the first pass at every alert with transparent, auditable output, SOAR focused on actions rather than
              reasoning, and processes that remain analyst-centric even when automation could safely handle much of the work.
            </td>
          </tr>
          <tr>
            <td><strong>Alert fatigue and intentional suppression of detections to keep workload manageable</strong></td>
            <td>
              To cope with unsustainable alert volume, teams suppress detections they would otherwise enable, especially for cloud and
              identity, increasing blind spots where attackers can move.
            </td>
            <td>
              Investigation capacity limited by humans, no mechanism to offload extra detections to an AI investigator that can filter
              and escalate only what matters, and historical attempts to do this with brittle rules and scripts.
            </td>
          </tr>
          <tr>
            <td><strong>Difficulty turning SOC operations into a clear, defensible ROI story</strong></td>
            <td>
              CISOs struggle to show how SOC investments reduce risk or cost because most metrics are operational rather than financial.
              An Agentic SOC can surface concrete metrics like investigation time reductions, throughput gains, and SIEM cost savings.
            </td>
            <td>
              Reporting focused on activity instead of outcomes, no consistent unit economics model for SOC work, and limited visibility
              into how much human time and risk is actually reduced by existing tooling.
            </td>
          </tr>
        </tbody>
      </table>
    </div>
  </section>

  <!-- Section 2: Territory strategy -->
  <section>
    <div class="kicker">Territory</div>
    <h2 class="section-title">How I’d approach Minnesota & Iowa for Prophet</h2>
    <div class="two-column">
      <div>
        <p>
          For an Agentic SOC like Prophet, I would target large enterprises and upper mid-market organizations in Minnesota and Iowa
          with complex environments, multiple security tools, and visible pain around alert volume, MTTR, and SIEM cost.
        </p>
        <p style="margin-top:0.75rem;">
          My first step would be to segment the territory into a focused list of 10–20 anchor accounts where an AI SOC can clearly
          move the needle: organizations with a staffed SOC, high telemetry volume, and a desire to reduce manual investigation and
          MDR dependence.
        </p>
      </div>
      <div class="card-list">
        <article class="card">
          <div class="pill">Segmentation</div>
          <h3>Pick the right anchors</h3>
          <p>
            Start with Fortune‑500/1000‑scale organizations and large regionals in sectors like healthcare, finance, manufacturing,
            retail, and utilities, then filter for SOC maturity, SIEM/EDR footprint, and publicly visible hiring for SOC roles.
          </p>
        </article>
        <article class="card">
          <div class="pill">Signals</div>
          <h3>Use public signals as clues</h3>
          <p>
            Look at open SOC and IR roles, job posts mentioning SIEM, SOAR, and EDR platforms, any public statements about SOC
            modernization or MDR use, and sector breach trends, then test Prophet against those pains.
          </p>
        </article>
        <article class="card">
          <div class="pill">Motion</div>
          <h3>Run a problem‑led motion</h3>
          <p>
            Lead with specific problems like investigation time, MTTR, SIEM cost, and MDR frustration, then show how an Agentic SOC
            changes the math on analyst hours, risk, and tool ROI.
          </p>
        </article>
      </div>
    </div>
  </section>

  <!-- Section 3: Account research framework -->
  <section>
    <div class="kicker">Account research</div>
    <h2 class="section-title">How I’d build an Agentic SOC account plan</h2>
    <p class="intro">
      For Prophet, I would build a structured view of each target account that ties their SOC reality to specific Agentic SOC outcomes.
      In practice this lives in a spreadsheet or CRM, but this table shows the fields I’d track for a top‑10 list.
    </p>

    <div style="overflow-x:auto;">
      <table>
        <thead>
          <tr>
            <th>Account (example)</th>
            <th>Environment / HQ</th>
            <th>Signals I’d look for</th>
            <th>Agentic SOC problem to test</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Large MN healthcare system</td>
            <td>Healthcare · Minnesota · multi‑site, EHR, cloud</td>
            <td>
              Public breaches in sector, hiring for SOC and IR roles, job posts mentioning SIEM/EDR and cloud security, and any talk
              of 24x7 coverage or MDR.
            </td>
            <td>
              Alert fatigue, high investigation times, and SIEM cost in a highly regulated environment, where an Agentic SOC can cut
              MTTR and manual hours while improving coverage.
            </td>
          </tr>
          <tr>
            <td>Regional financial services HQ</td>
            <td>Banking / insurance · Minnesota</td>
            <td>
              Open SOC, TI, and fraud‑adjacent roles, references to SOC modernization, and regulatory focus on cyber and fraud risk.
            </td>
            <td>
              Need to reduce manual investigations and MDR dependence while increasing transparency and auditability of SOC operations.
            </td>
          </tr>
          <tr>
            <td>Midwest manufacturing leader</td>
            <td>Manufacturing · Minnesota or Iowa · IT/OT mix</td>
            <td>
              News about ransomware in sector, job posts linking OT and IT security, and SIEM/EDR tooling across plants and corporate.
            </td>
            <td>
              High volume of alerts from mixed IT/OT tools and limited SOC headcount, where an Agentic SOC can scale investigations
              without adding large numbers of analysts.
            </td>
          </tr>
        </tbody>
      </table>
    </div>

    <p style="margin-top:1rem; font-size:0.9rem;">
      In my own planning, I’d build this out for specific Minnesota and Iowa enterprises, then map MEDDPICC fields on top of it:
      metrics (investigation time, MTTR, SIEM cost), economic buyers (CISO, VP Security Ops), champions, and decision process
      for each logo.
    </p>
  </section>
</main>
</body>
</html>
